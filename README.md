# Information Aware Early Exit Neural Network Training
PRP project about Early Exit Neural Network(EENN).

Try an new idea using three different networks.

Deep Neural Network (DNN) is a powerful tool for automatically learning features from data, and it is proficient at learning tasks. However, it often comes with high computational costs and latency. Early exit is a way to alleviate these issues, allowing easy samples to bypass the entire process while achieving even higher accuracy and significantly lower latency than before. However, traditional early exit approaches require extensive hyperparameter fine-tuning, such as adjusting loss weights, and waste the early exit information during training. To address these issues, we propose a two-stage training strategy, a novel approach for EENN (Early Exit Neural Networks) that incorporates early exit mechanisms into the training process. This approach allows us to take advantage of the early exit information without the need for fine-tuning loss function weights. It can be applied to the training of any EENN. We evaluate this approach using three well-known networks, including LeNet, AlexNet, and ResNet, on datasets MNIST and CIFAR-10. By comparing its results with traditional methods, we observe improved performance and more appropriate sample segregation.
